message = FALSE
)
df_auto <- read.table("auto-mpg.data", quote="\"", comment.char="")
# Renomeando as colunas
colnames(df_auto) <- c("mpg", "cylinders", "displacement", "horsepower",
"weight", "acceleration", "model_year", "origin", "car_name")
# Definindo as colunas numéricas e categóricas
colunas_num <- c("mpg", "displacement", "horsepower", "weight", "acceleration")
colunas_categoricas <- c("cylinders", "model_year", "origin", "car_name")
# Convertendo colunas categóricas
df_auto[colunas_categoricas] <- lapply(df_auto[colunas_categoricas], as.factor)
# Convertendo colunas numéricas
df_auto[colunas_num] <- lapply(df_auto[colunas_num], as.numeric)
# separa marca do modelo
library(tidyverse)
df_auto <- df_auto %>%
separate(car_name, into = c("marca", "modelo"), sep = " ", extra = "merge") %>%
mutate(marca = as.factor(marca), modelo = as.factor(modelo))
summary(df_auto)
colunas_categoricas <- c("cylinders", "model_year", "origin", "marca", "modelo")
# Usando lapply para gerar os histogramas
#lapply(colunas_num, function(col) {
#  hist(df_auto[[col]], main = paste("Histograma de", col), xlab = col, col = "lightblue", border = "black")
#})
# Usando lapply para gerar os histogramas com informações no topo das colunas
invisible(lapply(colunas_num, function(col) {
# Gerando o histograma e armazenando o objeto
h <- hist(df_auto[[col]], main = paste("Histograma de", col),
xlab = col, col = "lightblue", border = "black", plot = FALSE)
# Desenhando o histograma
hist(df_auto[[col]], main = paste("Histograma de", col),
xlab = col, col = "lightblue", border = "black")
# Adicionando as informações no topo das colunas
text(h$mids, h$counts, labels = h$counts, pos = 3, cex = 0.8, col = "red")  # Mostra os counts
text(h$mids, h$density, labels = round(h$density, 2), pos = 1, cex = 0.8, col = "blue")  # Mostra a densidade
}))
# Usando lapply para gerar os boxplots
#lapply(colunas_num, function(col) {
#  boxplot(df_auto[[col]], main = paste("Boxplot de", col), xlab = col, col = "lightblue", border = "black")
#})
# Usando lapply para gerar os boxplots com informações de quartis e outros dados
invisible(lapply(colunas_num, function(col) {
# Calculando as estatísticas do boxplot
box_stats <- boxplot(df_auto[[col]], plot = FALSE)
# Gerando o boxplot
boxplot(df_auto[[col]], main = paste("Boxplot de", col),
ylab = col, col = "lightblue", border = "black")
# Adicionando as informações no gráfico
# Exibindo o mínimo, Q1, mediana, Q3, e máximo
text(1, box_stats$stats[1], labels = paste("Mínimo: ", round(box_stats$stats[1], 2)), pos = 3, cex = 0.8, col = "red")
text(1, box_stats$stats[2], labels = paste("Q1: ", round(box_stats$stats[2], 2)), pos = 3, cex = 0.8, col = "blue")
text(1, box_stats$stats[3], labels = paste("Mediana: ", round(box_stats$stats[3], 2)), pos = 3, cex = 0.8, col = "black")
text(1, box_stats$stats[4], labels = paste("Q3: ", round(box_stats$stats[4], 2)), pos = 3, cex = 0.8, col = "blue")
text(1, box_stats$stats[5], labels = paste("Máximo: ", round(box_stats$stats[5], 2)), pos = 3, cex = 0.8, col = "red")
}))
# Usando um loop for para gerar os resumos das colunas numéricas com seus nomes
for (col in colunas_num) {
cat("\nResumo da coluna:", col, "\n")  # Exibe o nome da coluna
print(summary(df_auto[[col]]))  # Exibe o resumo estatístico da coluna
}
# Loop para calcular frequências e criar gráficos para cada variável categórica
colunas_categoricas <- c("model_year", "origin", "marca")
for (var in colunas_categoricas) {
# Calcular as frequências
frequencias <- as.data.frame(table(df_auto[[var]]))
colnames(frequencias) <- c("categoria", "frequencia")
# Criar o gráfico de colunas
plot <- ggplot(frequencias, aes(x = categoria, y = frequencia)) +
geom_col(fill = "steelblue") +
geom_text(aes(label = frequencia), vjust = -0.5) +  # Adiciona rótulos acima das barras
labs(
title = paste("Gráfico de Colunas -", var),  # Nome da variável no título
x = "Categoria",
y = "Frequência"
) +
theme_minimal()
# Exibir o gráfico
print(plot)
}
# Converter a coluna 'cylinders' para numérica
df_auto$cylinders <- as.numeric(as.character(df_auto$cylinders))
# Criar o gráfico de dispersão com jitter e boxplots
ggplot(df_auto, aes(x = as.factor(cylinders), y = mpg)) +  # Usar as.factor para boxplot
geom_boxplot(width = 0.5, alpha = 0.5, outlier.shape = NA) +  # Boxplot sem outliers
geom_jitter(aes(color = origin),width = 0.15, height = 0, alpha = 0.45) +  # Pontos de dispersão
labs(
title = "Gráfico de Dispersão e Boxplot de MPG vs Cilindradas",
x = "Cilindradas",
y = "Milhas por Galão (MPG)"
) +
theme_minimal()
df_auto$cylinders <- as.factor(df_auto$cylinders)
library(ggplot2)
library(rlang)
colunas <- colunas_num[-1]
# Loop para criar gráficos de dispersão
for (col in colunas) {
# Criar o gráfico
plot <- ggplot(df_auto, aes(x = .data[[col]], y = mpg, color = origin)) +
geom_point() +  # Adiciona os pontos ao gráfico
labs(
title = paste("Gráfico de Dispersão de MPG vs", col),
x = col,
y = "Milhas por Galão (MPG)"
) +
theme_minimal()
# Exibir o gráfico
print(plot)  # Corrige o erro chamando explicitamente o objeto do gráfico
}
library(PerformanceAnalytics)
analise_correl <- select(df_auto, horsepower, displacement, weight, acceleration)
chart.Correlation((analise_correl),histogram=TRUE)
library(ggplot2)
library(FactoMineR)
library(factoextra)
# Selecionar as variáveis de interesse
df_pca <- df_auto[, c("horsepower", "mpg", "displacement", "weight", "acceleration")]
# Remover linhas com valores ausentes (se necessário)
df_pca <- na.omit(df_pca)
# Padronizar as variáveis (opcional, mas recomendado)
df_pca <- scale(df_pca)
# Aplicar a PCA
pca_result <- prcomp(df_pca, scale = TRUE)
# Resumo dos resultados
summary(pca_result)
# Gráfico de scree plot
fviz_eig(pca_result)
# Gráfico das variáveis
fviz_pca_var(pca_result, col.var = "contrib", gradient.cols = c("blue", "yellow", "red"), repel = TRUE)
# Gráfico dos indivíduos
fviz_pca_ind(pca_result, col.ind = "cos2", gradient.cols = c("blue", "yellow", "red"), repel = TRUE)
library(car)
df_auto_modelo <- df_auto %>%
select(mpg, weight, acceleration, model_year)
# Remover linhas com valores ausentes (se necessário)
df_auto_modelo <- na.omit(df_auto_modelo)
df_auto_modelo$weight <- df_auto_modelo$weight / 1000
# Calcula o lambda ótimo usando powerTransform (modelo sem covariáveis: mpg ~ 1)
lambda <- powerTransform(mpg ~ 1, data = df_auto_modelo)
print(summary(lambda))  # Verifique o resultado e o valor de lambda
# Extrai o lambda ótimo
lambda_opt <- lambda$lambda
cat("Lambda ótimo:", lambda_opt, "\n")
# Aplica a transformação Box-Cox na variável mpg usando bcPower
df_auto_modelo$mpg_boxcox <- bcPower(df_auto_modelo$mpg, lambda_opt)
## Define o dataframe e variáveis que farão parte do modelo
df_auto_modelo <- df_auto_modelo %>%
select(mpg_boxcox, weight, acceleration, model_year)
# Criar variáveis dummy para as variáveis categóricas
#reordenando para remover a dummy de maior volume
df_auto_modelo$model_year <- factor(df_auto_modelo$model_year, levels = c("73", "70", "71", "72", "74", "75", "76", "77", "78", "79", "80", "81", "82"))
# Criar variáveis dummy removendo uma categoria de referência "73"
df_auto_modelo_dummies <- df_auto_modelo %>%
model.matrix(~ . , data = .) %>%
as.data.frame()
# Remove a coluna intercept
df_auto_modelo_dummies <- df_auto_modelo_dummies %>% select(-`(Intercept)`)
#modelo nulo
lm_mpg_nulo <- lm(mpg_boxcox ~ 1, data=df_auto_modelo_dummies) # modelo nulo
summary(lm_mpg_nulo)
lm_mpg_full <- lm(mpg_boxcox ~ weight + acceleration + model_year70 + model_year71 + model_year72 +
model_year74 + model_year75 + model_year76 + model_year77 + model_year78 +
model_year79 + model_year80 + model_year81 + model_year82,
data = df_auto_modelo_dummies) # modelo com todas as variáveis
summary(lm_mpg_full)
# Calcular o VIF
vif_valores <- vif(lm_mpg_full)
print(vif_valores)
# Step partindo do modelo nulo até o modelo completo
forw <- step(lm_mpg_nulo, scope=list(lower=lm_mpg_nulo, upper=lm_mpg_full), direction = "forward")
summary(forw)
# melhor AIC
lm_mpg_aic <- lm(mpg_boxcox ~ weight + model_year80 + model_year82 +
model_year81 + model_year79 + model_year78 + model_year77 +
model_year76 + model_year74 + model_year75 + model_year71 +
acceleration, data = df_auto_modelo_dummies)
summary(lm_mpg_aic)
library(olsrr)
ols_vif_tol(lm_mpg_aic)
media_residuos <- mean(residuals(lm_mpg_aic))
print(media_residuos) # Deve ser próximo de 0
hist(residuals(lm_mpg_aic), main = "Histograma dos Resíduos", xlab = "Resíduos")
# Teste de normalidade (Shapiro-Wilk)
resultado <- shapiro.test(residuals(lm_mpg_aic))
print(resultado$p.value)
# Não passou no teste de normalidade p-valor > 0.05
# Verificar usando o teste de normalidade do KolmogorovSmirnov
residuos <- residuals(lm_mpg_aic)
desvio_residuos <- sd(residuos)
# Realiza o teste de Kolmogorov-Smirnov para verificar a normalidade
ks_test <- ks.test(residuos, "pnorm", mean = media_residuos, sd = desvio_residuos)
# Exibe o resultado do teste
print(ks_test)
qqnorm(residuals(lm_mpg_aic), main = "QQ Plot dos Resíduos")
qqline(residuals(lm_mpg_aic), col = "red")
# gráfico dos resíduos padronizados
residuos_padronizados <- rstandard(lm_mpg_aic)
plot(residuos_padronizados, main = "Resíduos Padronizados", ylab = "Resíduos Padronizados")
abline(h = c(-3, 3), col = "red") # Limites para outliers
library(lmtest)
bptest(lm_mpg_aic)
# Extrair os resíduos e os valores ajustados
residuos <- residuals(lm_mpg_aic)
valores_ajustados <- fitted(lm_mpg_aic)
# Criar o gráfico
plot(valores_ajustados, residuos,
xlab = "Valores Ajustados",
ylab = "Resíduos",
main = "Resíduos vs. Valores Ajustados",
pch = 20, col = "blue")  # Personalização dos pontos
# Adicionar uma linha horizontal em y = 0
abline(h = 0, col = "red", lwd = 2)
# Criar um data frame com os primeiros PCs (por exemplo, PC1 e PC2)
df_pca <- as.data.frame(pca_resultado$x[, 1:2])  # Seleciona os dois primeiros PCs
View(df_pca)
summary(pca_result)
# Criar um data frame com os componentes principais
# Aqui, selecionamos os primeiros PCs que explicam a maior parte da variância
df_pca_model <- as.data.frame(pca_result$x[, 1:3])  # Seleciona os 3 primeiros PCs
colnames(df_pca_model) <- c("PC1", "PC2", "PC3")    # Renomeia os PCs para facilitar
df_pca_model$mpg <- df_auto$mpg                     # Adiciona a variável dependente (mpg)
# Selecionar as variáveis de interesse
df_pca <- df_auto[, c("horsepower", "mpg", "displacement", "weight", "acceleration")]
# Remover linhas com valores ausentes (sincronizar com df_auto)
df_pca <- na.omit(df_pca)  # Remove linhas com NA
df_auto_limpo <- df_auto[complete.cases(df_pca), ]  # Remover as mesmas linhas do df_auto
# Padronizar as variáveis (opcional, mas recomendado)
df_pca <- scale(df_pca)
# Aplicar a PCA
pca_result <- prcomp(df_pca, scale = TRUE)
# Criar um data frame com os componentes principais
df_pca_model <- as.data.frame(pca_result$x[, 1:3])  # Seleciona os 3 primeiros PCs
colnames(df_pca_model) <- c("PC1", "PC2", "PC3")    # Renomeia os PCs para facilitar
# Adicionar a variável dependente (mpg) ao data frame sincronizado
df_pca_model$mpg <- df_auto_limpo$mpg  # Agora os tamanhos são iguais
summary(df_auto)
# Remover as linhas com valores ausentes (NA) do data frame df_auto
df_auto_limpo <- na.omit(df_auto)
# Verificar o número de linhas antes e depois da remoção de NA
n_linhas_antes <- nrow(df_auto)
n_linhas_depois <- nrow(df_auto_limpo)
cat("Linhas antes da remoção de NA:", n_linhas_antes, "\n")
cat("Linhas depois da remoção de NA:", n_linhas_depois, "\n")
# Selecionar as variáveis numéricas para a PCA
df_pca <- df_auto_limpo[, c("horsepower", "mpg", "displacement", "weight", "acceleration")]
# Padronizar as variáveis (opcional, mas recomendado para PCA)
df_pca <- scale(df_pca)
# Aplicar a PCA
pca_result <- prcomp(df_pca, scale = TRUE)
# Resumo dos resultados da PCA
summary(pca_result)
# Criar um data frame com os componentes principais
df_pca_model <- as.data.frame(pca_result$x[, 1:3])  # Seleciona os 3 primeiros PCs
colnames(df_pca_model) <- c("PC1", "PC2", "PC3")    # Renomeia os PCs para facilitar
# Adicionar a variável dependente (mpg) ao data frame sincronizado
df_pca_model$mpg <- df_auto_limpo$mpg
# Ajustar o modelo de regressão linear usando os PCs como preditores
modelo_pca <- lm(mpg ~ PC1 + PC2 + PC3, data = df_pca_model)
# Resumo do modelo
summary(modelo_pca)
# Diagnóstico do modelo
par(mfrow = c(2, 2))  # Configura layout para múltiplos gráficos
plot(modelo_pca)
# Teste de normalidade dos resíduos
shapiro.test(residuals(modelo_pca))
# Teste de homocedasticidade (Breusch-Pagan)
library(lmtest)
bptest(modelo_pca)
# Remover as linhas com valores ausentes (NA) do data frame df_auto
df_auto_limpo <- na.omit(df_auto)
# Verificar o número de linhas antes e depois da remoção de NA
n_linhas_antes <- nrow(df_auto)
n_linhas_depois <- nrow(df_auto_limpo)
cat("Linhas antes da remoção de NA:", n_linhas_antes, "\n")
cat("Linhas depois da remoção de NA:", n_linhas_depois, "\n")
# Selecionar as variáveis numéricas para a PCA
df_pca <- df_auto_limpo[, c("horsepower", "mpg", "displacement", "weight", "acceleration")]
# Padronizar as variáveis (opcional, mas recomendado para PCA)
df_pca <- scale(df_pca)
# Aplicar a PCA
pca_result <- prcomp(df_pca, scale = TRUE)
# Resumo dos resultados da PCA
summary(pca_result)
# Criar um data frame com os componentes principais
df_pca_model <- as.data.frame(pca_result$x[, 1:3])  # Seleciona os 3 primeiros PCs
colnames(df_pca_model) <- c("PC1", "PC2", "PC3")    # Renomeia os PCs para facilitar
# Adicionar a variável dependente (mpg) ao data frame sincronizado
df_pca_model$mpg <- df_auto_limpo$mpg
# Ajustar o modelo de regressão linear usando os PCs como preditores
modelo_pca <- lm(mpg ~ PC1 + PC2 + PC3, data = df_pca_model)
# Resumo do modelo
summary(modelo_pca)
# Diagnóstico do modelo
#par(mfrow = c(2, 2))  # Configura layout para múltiplos gráficos
plot(modelo_pca)
# Teste de normalidade dos resíduos
shapiro.test(residuals(modelo_pca))
# Teste de homocedasticidade (Breusch-Pagan)
library(lmtest)
bptest(modelo_pca)
plot(lm_mpg_aic)
# Remover as linhas com valores ausentes (NA) do data frame df_auto
df_auto_limpo <- na.omit(df_auto)
# Selecionar as variáveis numéricas para a PCA
df_pca <- df_auto_limpo[, c("horsepower", "mpg", "displacement", "weight", "acceleration")]
# Padronizar as variáveis (opcional, mas recomendado para PCA)
df_pca <- scale(df_pca)
# Aplicar a PCA
pca_result <- prcomp(df_pca, scale = TRUE)
# Resumo dos resultados da PCA
summary(pca_result)
# Criar um data frame com os componentes principais
df_pca_model <- as.data.frame(pca_result$x[, 1:3])  # Seleciona os 3 primeiros PCs
colnames(df_pca_model) <- c("PC1", "PC2", "PC3")    # Renomeia os PCs para facilitar
# Adicionar a variável dependente (mpg) ao data frame sincronizado
df_pca_model$mpg <- df_auto_limpo$mpg
# Ajustar o modelo de regressão linear usando os PCs como preditores
modelo_pca <- lm(mpg ~ PC1 + PC2 + PC3, data = df_pca_model)
# Resumo do modelo
summary(modelo_pca)
# Diagnóstico do modelo
#par(mfrow = c(2, 2))  # Configura layout para múltiplos gráficos
plot(modelo_pca)
# Teste de normalidade dos resíduos
shapiro.test(residuals(modelo_pca))
# Teste de homocedasticidade (Breusch-Pagan)
library(lmtest)
bptest(modelo_pca)
# Remover as linhas com valores ausentes (NA) do data frame df_auto
df_auto_limpo <- na.omit(df_auto)
# Selecionar as variáveis numéricas para a PCA
df_pca <- df_auto_limpo[, c("horsepower", "mpg", "displacement", "weight", "acceleration")]
# Padronizar as variáveis (opcional, mas recomendado para PCA)
df_pca <- scale(df_pca)
# Aplicar a PCA
pca_result <- prcomp(df_pca, scale = TRUE)
# Resumo dos resultados da PCA
summary(pca_result)
# Criar um data frame com os componentes principais
df_pca_model <- as.data.frame(pca_result$x[, 1:3])  # Seleciona os 3 primeiros PCs
colnames(df_pca_model) <- c("PC1", "PC2", "PC3")    # Renomeia os PCs para facilitar
# Adicionar a variável dependente (mpg) ao data frame sincronizado
df_pca_model$mpg <- df_auto_limpo$mpg
# Ajustar o modelo de regressão linear usando os PCs como preditores
modelo_pca <- lm(mpg ~ PC1 + PC2 + PC3, data = df_pca_model)
# Resumo do modelo
summary(modelo_pca)
# Diagnóstico do modelo
#par(mfrow = c(2, 2))  # Configura layout para múltiplos gráficos
plot(modelo_pca)
# Teste de normalidade dos resíduos
shapiro.test(residuals(modelo_pca))
# Teste de homocedasticidade (Breusch-Pagan)
library(lmtest)
bptest(modelo_pca)
# Remover as linhas com valores ausentes (NA) do data frame df_auto
df_auto_limpo <- na.omit(df_auto)
# Selecionar as variáveis numéricas para a PCA
df_pca <- df_auto_limpo[, c("horsepower", "mpg", "displacement", "weight", "acceleration")]
# Padronizar as variáveis (opcional, mas recomendado para PCA)
df_pca <- scale(df_pca)
# Aplicar a PCA
pca_result <- prcomp(df_pca, scale = TRUE)
# Resumo dos resultados da PCA
summary(pca_result)
# Criar um data frame com os componentes principais
df_pca_model <- as.data.frame(pca_result$x[, 1:3])  # Seleciona os 3 primeiros PCs
colnames(df_pca_model) <- c("PC1", "PC2", "PC3")    # Renomeia os PCs para facilitar
# Adicionar a variável dependente (mpg) ao data frame sincronizado
df_pca_model$mpg <- df_auto_limpo$mpg
# Ajustar o modelo de regressão linear usando os PCs como preditores
modelo_pca <- lm(mpg ~ PC1 + PC2 + PC3, data = df_pca_model)
# Resumo do modelo
summary(modelo_pca)
# Diagnóstico do modelo
#par(mfrow = c(2, 2))  # Configura layout para múltiplos gráficos
plot(modelo_pca)
# Teste de normalidade dos resíduos
shapiro.test(residuals(modelo_pca))
# Teste de homocedasticidade (Breusch-Pagan)
library(lmtest)
bptest(modelo_pca)
# Remover as linhas com valores ausentes (NA) do data frame df_auto
df_auto_limpo <- na.omit(df_auto)
# Selecionar as variáveis numéricas para a PCA
df_pca <- df_auto_limpo[, c("horsepower", "mpg", "displacement", "weight", "acceleration")]
# Padronizar as variáveis (opcional, mas recomendado para PCA)
df_pca <- scale(df_pca)
# Aplicar a PCA
pca_result <- prcomp(df_pca, scale = TRUE)
# Resumo dos resultados da PCA
summary(pca_result)
# Criar um data frame com os componentes principais
df_pca_model <- as.data.frame(pca_result$x[, 1:3])  # Seleciona os 3 primeiros PCs
colnames(df_pca_model) <- c("PC1", "PC2", "PC3")    # Renomeia os PCs para facilitar
# Adicionar a variável dependente (mpg) ao data frame sincronizado
df_pca_model$mpg <- df_auto_limpo$mpg
# Ajustar o modelo de regressão linear usando os PCs como preditores
modelo_pca <- lm(mpg ~ PC1 + PC2 + PC3, data = df_pca_model)
# Resumo do modelo
summary(modelo_pca)
# Diagnóstico do modelo
#par(mfrow = c(2, 2))  # Configura layout para múltiplos gráficos
plot(modelo_pca)
# Teste de homocedasticidade (Breusch-Pagan)
library(lmtest)
bptest(modelo_pca)
# Calcular os valores ajustados pelo modelo com AIC
valores_ajustados_aic <- fitted(lm_mpg_aic)
# Calcular o RMSE para o modelo com AIC
rmse_aic <- sqrt(mean((df_auto$mpg - valores_ajustados_aic)^2))
cat("RMSE do modelo com AIC:", rmse_aic, "\n")
# Calcular os valores ajustados pelo modelo com PCA
valores_ajustados_pca <- fitted(modelo_pca)
# Calcular o RMSE para o modelo com PCA
rmse_pca <- sqrt(mean((df_pca_model$mpg - valores_ajustados_pca)^2))
cat("RMSE do modelo com PCA:", rmse_pca, "\n")
r2_aic <- summary(lm_mpg_aic)$r.squared
r2_pca <- summary(modelo_pca)$r.squared
cat("R² do modelo com AIC:", r2_aic, "\n")
cat("R² do modelo com PCA:", r2_pca, "\n")
# Modelo com AIC
mae_aic <- mean(abs(df_auto$mpg - fitted(lm_mpg_aic)))
# Modelo com PCA
mae_pca <- mean(abs(df_pca_model$mpg - fitted(modelo_pca)))
cat("MAE do modelo com AIC:", mae_aic, "\n")
cat("MAE do modelo com PCA:", mae_pca, "\n")
# Modelo com AIC
mse_aic <- mean((df_auto$mpg - fitted(lm_mpg_aic))^2)
# Modelo com PCA
mse_pca <- mean((df_pca_model$mpg - fitted(modelo_pca))^2)
cat("MSE do modelo com AIC:", mse_aic, "\n")
cat("MSE do modelo com PCA:", mse_pca, "\n")
cat("RMSE do modelo com AIC:", rmse_aic, "\n")
cat("RMSE do modelo com PCA:", rmse_pca, "\n")
# Modelo com AIC
mape_aic <- mean(abs((df_auto$mpg - fitted(lm_mpg_aic)) / df_auto$mpg)) * 100
# Modelo com PCA
mape_pca <- mean(abs((df_pca_model$mpg - fitted(modelo_pca)) / df_pca_model$mpg)) * 100
cat("MAPE do modelo com AIC:", mape_aic, "%\n")
cat("MAPE do modelo com PCA:", mape_pca, "%\n")
# Para o modelo com AIC
plot(fitted(lm_mpg_aic), residuals(lm_mpg_aic),
xlab = "Valores Ajustados", ylab = "Resíduos",
main = "Resíduos vs Valores Ajustados (AIC)")
abline(h = 0, col = "red")
# Para o modelo com PCA
plot(fitted(modelo_pca), residuals(modelo_pca),
xlab = "Valores Ajustados", ylab = "Resíduos",
main = "Resíduos vs Valores Ajustados (PCA)")
abline(h = 0, col = "red")
# Calcular os valores ajustados pelo modelo com AIC
valores_ajustados_aic <- fitted(lm_mpg_aic)
# Calcular o RMSE para o modelo com AIC
rmse_aic <- sqrt(mean((df_auto$mpg - valores_ajustados_aic)^2))
cat("RMSE do modelo com AIC:", rmse_aic, "\n")
# Calcular os valores ajustados pelo modelo com PCA
valores_ajustados_pca <- fitted(modelo_pca)
# Calcular o RMSE para o modelo com PCA
rmse_pca <- sqrt(mean((df_pca_model$mpg - valores_ajustados_pca)^2))
cat("RMSE do modelo com PCA:", rmse_pca, "\n")
r2_aic <- summary(lm_mpg_aic)$r.squared
r2_pca <- summary(modelo_pca)$r.squared
cat("R² do modelo com AIC:", r2_aic, "\n")
cat("R² do modelo com PCA:", r2_pca, "\n")
# Modelo com AIC
mae_aic <- mean(abs(df_auto$mpg - fitted(lm_mpg_aic)))
# Modelo com PCA
mae_pca <- mean(abs(df_pca_model$mpg - fitted(modelo_pca)))
cat("MAE do modelo com AIC:", mae_aic, "\n")
cat("MAE do modelo com PCA:", mae_pca, "\n")
# Modelo com AIC
mse_aic <- mean((df_auto$mpg - fitted(lm_mpg_aic))^2)
# Modelo com PCA
mse_pca <- mean((df_pca_model$mpg - fitted(modelo_pca))^2)
cat("MSE do modelo com AIC:", mse_aic, "\n")
cat("MSE do modelo com PCA:", mse_pca, "\n")
cat("RMSE do modelo com AIC:", rmse_aic, "\n")
cat("RMSE do modelo com PCA:", rmse_pca, "\n")
# Modelo com AIC
mape_aic <- mean(abs((df_auto$mpg - fitted(lm_mpg_aic)) / df_auto$mpg)) * 100
# Modelo com PCA
mape_pca <- mean(abs((df_pca_model$mpg - fitted(modelo_pca)) / df_pca_model$mpg)) * 100
cat("MAPE do modelo com AIC:", mape_aic, "%\n")
cat("MAPE do modelo com PCA:", mape_pca, "%\n")
# Para o modelo com AIC
plot(fitted(lm_mpg_aic), residuals(lm_mpg_aic),
xlab = "Valores Ajustados", ylab = "Resíduos",
main = "Resíduos vs Valores Ajustados (AIC)")
abline(h = 0, col = "red")
# Para o modelo com PCA
plot(fitted(modelo_pca), residuals(modelo_pca),
xlab = "Valores Ajustados", ylab = "Resíduos",
main = "Resíduos vs Valores Ajustados (PCA)")
abline(h = 0, col = "red")
library(ggplot2)
library(FactoMineR)
library(factoextra)
# Selecionar as variáveis de interesse
df_pca <- df_auto[, c("horsepower", "mpg", "displacement", "weight", "acceleration")]
# Remover linhas com valores ausentes (se necessário)
df_pca <- na.omit(df_pca)
# Padronizar as variáveis (opcional, mas recomendado)
df_pca <- scale(df_pca)
# Aplicar a PCA
pca_result <- prcomp(df_pca, scale = TRUE)
# Resumo dos resultados
summary(pca_result)
# Gráfico de scree plot
fviz_eig(pca_result)
# Gráfico das variáveis
fviz_pca_var(pca_result, col.var = "contrib", gradient.cols = c("blue", "yellow", "red"), repel = TRUE)
